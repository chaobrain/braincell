{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30a3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1226b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, typ, value, line_no):\n",
    "        self.type = typ    \n",
    "        self.value = value \n",
    "        self.line_no = line_no\n",
    "    def __repr__(self):\n",
    "        return f\"Token({self.type}, {self.value})\"\n",
    "\n",
    "def tokenize_asc_line(line, line_no):\n",
    "    \"\"\"\n",
    "    Tokenize a single line from a Neurolucida ASC file.\n",
    "\n",
    "    This function splits a line into tokens that represent different elements,\n",
    "    including parentheses, commas, numbers, quoted strings, reserved keywords,\n",
    "    and labels. It skips whitespace and comments, and supports various Neurolucida\n",
    "    keywords and constructs.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    length = len(line)\n",
    "    while i < length:\n",
    "        c = line[i]\n",
    "        # Skip whitespace\n",
    "        if c.isspace():\n",
    "            i += 1\n",
    "            continue\n",
    "        # Skip comments\n",
    "        if c == ';':\n",
    "            break\n",
    "        # Single-character symbols\n",
    "        if c == '(':\n",
    "            tokens.append(Token('leftpar', '(', line_no))\n",
    "            i += 1\n",
    "            continue\n",
    "        if c == ')':\n",
    "            tokens.append(Token('rightpar', ')', line_no))\n",
    "            i += 1\n",
    "            continue\n",
    "        if c == ',':\n",
    "            tokens.append(Token('comma', ',', line_no))\n",
    "            i += 1\n",
    "            continue\n",
    "        if c == '|':\n",
    "            tokens.append(Token('bar', '|', line_no))\n",
    "            i += 1\n",
    "            continue\n",
    "        if c == '<':\n",
    "            tokens.append(Token('leftsp', '<', line_no))\n",
    "            i += 1\n",
    "            continue\n",
    "        if c == '>':\n",
    "            tokens.append(Token('rightsp', '>', line_no))\n",
    "            i += 1\n",
    "            continue\n",
    "        # String with double quotes\n",
    "        if c == '\"':\n",
    "            j = i + 1\n",
    "            while j < length and line[j] != '\"':\n",
    "                j += 1\n",
    "            if j < length:\n",
    "                tokens.append(Token('string', line[i+1:j], line_no))\n",
    "                i = j + 1\n",
    "                continue\n",
    "            else:\n",
    "                tokens.append(Token('err_', line[i:], line_no))\n",
    "                break\n",
    "        # Keywords: set, Set, SET\n",
    "        if line[i:].startswith('set ') or line[i:].startswith('Set ') or line[i:].startswith('SET '):\n",
    "            tokens.append(Token('set', 'set', line_no))\n",
    "            i += line[i:].find(' ') + 1\n",
    "            continue\n",
    "        # Keyword: RGB\n",
    "        if line[i:].startswith('RGB '):\n",
    "            tokens.append(Token('rgb', 'RGB', line_no))\n",
    "            i += 4\n",
    "            continue\n",
    "        # Numbers\n",
    "        m = re.match(r'[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?', line[i:])\n",
    "        if m:\n",
    "            val = m.group(0)\n",
    "            tokens.append(Token('number', float(val), line_no))\n",
    "            i += len(val)\n",
    "            continue\n",
    "        # Label (identifiers)\n",
    "        m = re.match(r'[A-Za-z_][A-Za-z0-9_]*', line[i:])\n",
    "        if m:\n",
    "            tokens.append(Token('label_', m.group(0), line_no))\n",
    "            i += len(m.group(0))\n",
    "            continue\n",
    "        # Unrecognized character\n",
    "        tokens.append(Token('err_', c, line_no))\n",
    "        i += 1\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa63df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenStream:\n",
    "    \"\"\"\n",
    "    A simple stream wrapper around a list of tokens, providing\n",
    "    lookahead and cursor movement, used by the ASC parser.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens):\n",
    "        self.tokens = tokens\n",
    "        self.idx = 0\n",
    "\n",
    "    @property\n",
    "    def current(self):\n",
    "        \"\"\"Return the current token, or EOF if at the end.\"\"\"\n",
    "        return self.tokens[self.idx] if self.idx < len(self.tokens) else Token(\"eof\", None, -1)\n",
    "\n",
    "    @property\n",
    "    def look_ahead(self):\n",
    "        \"\"\"Return the next token, or EOF if at the end.\"\"\"\n",
    "        return self.tokens[self.idx+1] if self.idx+1 < len(self.tokens) else Token(\"eof\", None, -1)\n",
    "\n",
    "    @property\n",
    "    def look_ahead2(self):\n",
    "        \"\"\"Return the token after next, or EOF if at the end.\"\"\"\n",
    "        return self.tokens[self.idx+2] if self.idx+2 < len(self.tokens) else Token(\"eof\", None, -1)\n",
    "\n",
    "    def advance(self):\n",
    "        \"\"\"Advance the token pointer by one.\"\"\"\n",
    "        self.idx += 1\n",
    "\n",
    "    def expect(self, typ):\n",
    "        \"\"\"\n",
    "        Ensure the current token is of the expected type, advance, and return it.\n",
    "        Raise ValueError otherwise.\n",
    "        \"\"\"\n",
    "        if self.current.type != typ:\n",
    "            raise ValueError(f\"Expected {typ}, got {self.current}\")\n",
    "        tok = self.current\n",
    "        self.advance()\n",
    "        return tok\n",
    "\n",
    "    def is_eof(self):\n",
    "        \"\"\"Check if the stream has reached the end.\"\"\"\n",
    "        return self.current.type == 'eof'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d775d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contourcenter(points, num=101):\n",
    "    \"\"\"\n",
    "    Uniformly resample a 3D contour by arclength and return centroid.\n",
    "\n",
    "    Args:\n",
    "        points: list of Point (with .x, .y, .z)\n",
    "        num: number of resample points\n",
    "\n",
    "    Returns:\n",
    "        mean: ndarray, shape (3,), centroid (mean_x, mean_y, mean_z)\n",
    "        x_new, y_new, z_new: ndarray, resampled coordinates (length=num)\n",
    "    \"\"\"\n",
    "    x = np.array([p.x for p in points])\n",
    "    y = np.array([p.y for p in points])\n",
    "    z = np.array([p.z for p in points])\n",
    "    seglens = np.sqrt(np.diff(x)**2 + np.diff(y)**2 + np.diff(z)**2)\n",
    "    perim = np.zeros(len(x))\n",
    "    perim[1:] = np.cumsum(seglens)\n",
    "    d_uniform = np.linspace(0, perim[-1], num)\n",
    "    x_new = np.interp(d_uniform, perim, x)\n",
    "    y_new = np.interp(d_uniform, perim, y)\n",
    "    z_new = np.interp(d_uniform, perim, z)\n",
    "    mean = np.array([x_new.mean(), y_new.mean(), z_new.mean()])\n",
    "    return mean, x_new, y_new, z_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "febb659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    \"\"\"\n",
    "    Represents a 3D point in a neuron morphology, possibly with additional misc info.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, z, d, misc, idx):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "        self.d = d\n",
    "        self.idx = idx    # Global index for the point\n",
    "        self.misc = misc  # Additional labels or attributes\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.x, self.y, self.z) == (other.x, other.y, other.z)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Point({self.idx}: {self.x}, {self.y}, {self.z}, {self.d}, {self.misc})\"\n",
    "\n",
    "class Section:\n",
    "    \"\"\"\n",
    "    Represents a section (branch or soma) in the reconstructed morphology.\n",
    "    Contains a list of Point objects, type info, parent id, and a contour stack.\n",
    "    \"\"\"\n",
    "    def __init__(self, sec_id, sec_type, parent_id=None, parent_x = -1):\n",
    "        self.sec_id = sec_id          # Unique identifier for the section\n",
    "        self.sec_type = sec_type      # soma = 1, dend = 2, axon = 3 ...\n",
    "        self.points = []              # List of Point objects\n",
    "        self.parent_id = parent_id    # The parent section, or None if root\n",
    "        self.contour_stack = []       # Used for complex objects (e.g. multi-contour soma)\n",
    "        self.parent_x = parent_x\n",
    "        \n",
    "    @property\n",
    "    def z_range(self):\n",
    "        \"\"\"Return (min_z, max_z) of all points in section, or (0,0) if empty.\"\"\"\n",
    "        zs = [p.z for p in self.points]\n",
    "        return (min(zs), max(zs)) if zs else (0, 0)\n",
    "    \n",
    "    @property\n",
    "    def center(self):\n",
    "        mean,_,_,_ = contourcenter(self.points)\n",
    "        return (mean[0], mean[1])\n",
    "    \n",
    "    @property\n",
    "    def bbox_xy(self):\n",
    "        \"\"\"Return bounding box (min_x, max_x, min_y, max_y) in XY-plane.\"\"\"\n",
    "        xs = [p.x for p in self.points]\n",
    "        ys = [p.y for p in self.points]\n",
    "        return (min(xs), max(xs), min(ys), max(ys))\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def stk_bbox_xy(self):\n",
    "        \"\"\"\n",
    "        Return the bounding box (xmin, xmax, ymin, ymax) in the XY-plane,\n",
    "        including both main points and all points in the contour_stack.\n",
    "        \"\"\"\n",
    "        xs = [p.x for p in self.points]\n",
    "        ys = [p.y for p in self.points]\n",
    "        for contour in self.contour_stack:\n",
    "            xs += [p.x for p in contour]\n",
    "            ys += [p.y for p in contour]\n",
    "        return (min(xs), max(xs), min(ys), max(ys))\n",
    "\n",
    "    @property\n",
    "    def stk_center(self):\n",
    "        \"\"\"\n",
    "        Return geometric 'stack center' for the contour stack (multi-layer soma contour).\n",
    "        Returns: (x, y, z) tuple\n",
    "        \"\"\"\n",
    "        centers = []\n",
    "        # 1. Center of the main section point set (supports 3D)\n",
    "        mean, _, _, _ = contourcenter(self.points)\n",
    "        centers.append(tuple(mean))  # (x, y, z)\n",
    "\n",
    "        # 2. Each contour in contour_stack\n",
    "        for contour in getattr(self, 'contour_stack', []):\n",
    "            if contour:\n",
    "                # Support contour being either a list of points or Section\n",
    "                pts = getattr(contour, \"points\", contour)\n",
    "                mean, _, _, _ = contourcenter(pts)\n",
    "                centers.append(tuple(mean))  # (x, y, z)\n",
    "\n",
    "        # 3. Cumulative principal axis length and interpolation\n",
    "        lengths = [0.0]\n",
    "        for i in range(1, len(centers)):\n",
    "            dx = centers[i][0] - centers[i-1][0]\n",
    "            dy = centers[i][1] - centers[i-1][1]\n",
    "            dz = centers[i][2] - centers[i-1][2]\n",
    "            l = (dx**2 + dy**2 + dz**2)**0.5\n",
    "            lengths.append(lengths[-1] + l)\n",
    "        half_len = lengths[-1] / 2\n",
    "        if half_len == 0:\n",
    "            return centers[0]\n",
    "\n",
    "        for i in range(1, len(lengths)):\n",
    "            if lengths[i] > half_len:\n",
    "                th = (half_len - lengths[i-1]) / (lengths[i] - lengths[i-1])\n",
    "                c0 = centers[i-1]\n",
    "                c1 = centers[i]\n",
    "                center = (\n",
    "                    th * c1[0] + (1-th) * c0[0],\n",
    "                    th * c1[1] + (1-th) * c0[1],\n",
    "                    th * c1[2] + (1-th) * c0[2],\n",
    "                )\n",
    "                return center\n",
    "        return centers[-1]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"(Section(id={self.sec_id},type={self.sec_type},points={len(self.points)},pid={self.parent_id},px={self.parent_x})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a8662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    \"\"\"\n",
    "    The main parser for Neurolucida ASC files.\n",
    "    Handles parsing of all major ASC constructs, including contours (soma),\n",
    "    trees (dendrites/axons), properties, spines, and marker lists.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "        self.ts = TokenStream(tokens)     # Token stream\n",
    "\n",
    "        self.all_points = []              # Flat list of all parsed Point objects\n",
    "        self.sections = []                # List of parsed Section objects\n",
    "        self.cur_section_type = 0         # Type flag for new sections\n",
    "\n",
    "        self.spines = []                  # List of spine dicts\n",
    "        self.blocks = []                  # Optional: stores parse block names for debugging\n",
    "\n",
    "    def parse(self):\n",
    "        \"\"\"\n",
    "        Main entry: parse all tokens into ASC blocks.\n",
    "        Calls parse_object() for each recognized left parenthesis.\n",
    "        \"\"\"\n",
    "        while not self.ts.is_eof():\n",
    "            if self.ts.current.type == 'leftpar':\n",
    "                self.parse_object()\n",
    "            else:\n",
    "                self.ts.advance()\n",
    "        return self.blocks\n",
    "\n",
    "    def parse_object(self):\n",
    "        \"\"\"\n",
    "        Parse a high-level ASC block (contour, tree, set, marker, etc.)\n",
    "        Dispatch based on lookahead tokens.\n",
    "        \"\"\"\n",
    "        cur = self.ts.current\n",
    "        la = self.ts.look_ahead\n",
    "        la2 = self.ts.look_ahead2\n",
    "\n",
    "        #print(f\"parse_object @{self.ts.idx}: cur={cur} la={la} la2={la2}\")\n",
    "\n",
    "        # 1. Contour blocks: e.g. (\"Cell Body\" ...)\n",
    "        if self.ts.look_ahead.type == 'string':\n",
    "            self.blocks.append(f\"contour: {la.value}\")\n",
    "            return self.parse_contour()\n",
    "\n",
    "        # 2. Tree (axon/dendrite) or text block\n",
    "        if self.ts.look_ahead.type == 'leftpar':\n",
    "            self.blocks.append(f\"tree or text\")\n",
    "            return self.parse_tree_or_text()\n",
    "\n",
    "        # 3. Property blocks (Color, CellBody, Class, etc.)\n",
    "        if self.ts.look_ahead.type == 'label_' and self.ts.look_ahead2.type in ('number', 'string'):\n",
    "            self.blocks.append(f\"property: {la.value}\")\n",
    "            self.skip_unknown_block()\n",
    "            return\n",
    "\n",
    "        # 4. Set (metadata block)\n",
    "        if self.ts.look_ahead.type == 'set':\n",
    "            self.blocks.append(f\"set: {la.value}\")\n",
    "            self.skip_unknown_block()\n",
    "            return\n",
    "\n",
    "        # 5. Spine (special labeled block)\n",
    "        if self.ts.look_ahead.type == 'label_' and self.ts.look_ahead.value == 'Spine':\n",
    "            self.blocks.append(f\"spine: {la.value}\")\n",
    "            self.skip_unknown_block()\n",
    "            return\n",
    "\n",
    "        # 6. Marker block\n",
    "        if self.ts.look_ahead.type == 'label_' and self.ts.look_ahead.value == 'Marker':\n",
    "            self.blocks.append(f\"Maker: {la.value}\")\n",
    "            self.skip_unknown_block()\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            # Unknown or unrecognized block\n",
    "            return self.skip_unknown_block()\n",
    "\n",
    "    def skip_unknown_block(self):\n",
    "        \"\"\"\n",
    "        Skip over a complete parenthesized block (from current '(' to matching ')').\n",
    "        Used for skipping unrecognized or currently unhandled blocks.\n",
    "        \"\"\"\n",
    "        depth = 1\n",
    "        self.ts.advance()\n",
    "        while not self.ts.is_eof() and depth > 0:\n",
    "            if self.ts.current.type == 'leftpar':\n",
    "                depth += 1\n",
    "            elif self.ts.current.type == 'rightpar':\n",
    "                depth -= 1\n",
    "            self.ts.advance()\n",
    "\n",
    "    def parse_contour(self):\n",
    "        \"\"\"\n",
    "        Parse a (contour ...) block.\n",
    "        For soma: at least 3 points are required.\n",
    "        Handles possible attribute blocks, then points.\n",
    "        \"\"\"\n",
    "        self.ts.expect('leftpar')\n",
    "        string = self.ts.expect('string').value\n",
    "\n",
    "        begin = len(self.all_points)\n",
    "        attributes = []\n",
    "\n",
    "        while True:\n",
    "            if self.ts.current.type == 'rightpar':\n",
    "                break\n",
    "            # Property/attribute blocks: e.g. (Color Red)\n",
    "            if self.ts.current.type == 'leftpar':\n",
    "                if self.ts.look_ahead.type == 'label_':\n",
    "                    #print('self.parse_property()')\n",
    "                    attributes.append(self.parse_property())\n",
    "                elif self.ts.look_ahead.type == 'set':\n",
    "                    #print('self.parse_set()')\n",
    "                    self.skip_unknown_block()\n",
    "                elif self.ts.look_ahead.type == 'number':\n",
    "                    #print('self.point()')\n",
    "                    self.parse_point()\n",
    "                else:\n",
    "                    print(f\"Warning: Unexpected contour block {self.ts.current}, {self.ts.look_ahead}\")\n",
    "                    self.skip_unknown_block()\n",
    "            else:\n",
    "                print(f\"Skipping unexpected token {self.ts.current}\")\n",
    "                self.ts.advance()\n",
    "\n",
    "        # Set type for soma (cell body)\n",
    "        if string in [\"Cell Body\", \"CellBody\", \"Soma\"]:\n",
    "            self.cur_section_type = 1\n",
    "\n",
    "        end = len(self.all_points)\n",
    "        if end - begin > 2:\n",
    "            section = Section(sec_id=len(self.sections), sec_type=self.cur_section_type)\n",
    "            section.points = self.all_points[begin:end]\n",
    "            self.sections.append(section)\n",
    "        else:\n",
    "            raise ValueError(\"soma must at least has 3 points！\")\n",
    "\n",
    "        self.ts.expect('rightpar')\n",
    "        return {\n",
    "            \"type\": \"contour\",\n",
    "            \"name\": string,\n",
    "            \"attributes\": attributes,\n",
    "        }\n",
    "\n",
    "    def parse_property(self):\n",
    "        \"\"\"\n",
    "        Parse a (label_ ...) property/attribute block.\n",
    "        E.g. (Color Red), (Axon), (Class 1 'Spine').\n",
    "        Returns the label and values (can be numbers, strings, RGB, etc).\n",
    "        \"\"\"\n",
    "        self.ts.expect('leftpar')\n",
    "        label = self.ts.expect('label_').value\n",
    "\n",
    "        # Set section type by property label\n",
    "        if label == \"Axon\":\n",
    "            self.cur_section_type = 2\n",
    "        elif label == \"Dendrite\":\n",
    "            self.cur_section_type = 3\n",
    "        elif label == \"Apical\":\n",
    "            self.cur_section_type = 4\n",
    "        elif label == \"CellBody\" or label == \"Cell Body\" or label == \"Soma\":\n",
    "            self.cur_section_type = 1\n",
    "        values = []\n",
    "\n",
    "        # Parse all values until ')'\n",
    "        while not self.ts.is_eof() and self.ts.current.type != 'rightpar':\n",
    "            typ = self.ts.current.type\n",
    "            if typ in ('number', 'string', 'label_', 'rgb'):\n",
    "                values.append(self.ts.current.value)\n",
    "                self.ts.advance()\n",
    "\n",
    "                # Parse RGB triple, e.g. (RGB (1,0,0))\n",
    "                if typ.lower() == \"rgb\":\n",
    "                    if self.ts.current.type == 'leftpar':\n",
    "                        self.ts.advance()  # consume '('\n",
    "                    rgb_values = []\n",
    "                    while len(rgb_values) < 3:\n",
    "                        if self.ts.current.type == 'comma':\n",
    "                            self.ts.advance()\n",
    "                            continue\n",
    "                        if self.ts.current.type == 'number':\n",
    "                            rgb_values.append(self.ts.current.value)\n",
    "                            self.ts.advance()\n",
    "                        else:\n",
    "                            break\n",
    "                    if len(rgb_values) != 3:\n",
    "                        raise ValueError(\"RGB needs 3 numbers\")\n",
    "                    values.extend(rgb_values)\n",
    "                    if self.ts.current.type == 'rightpar':\n",
    "                        self.ts.advance()\n",
    "                    else:\n",
    "                        raise ValueError(f\"Parse error: RGB property not closed with ')', got {self.ts.current}\")\n",
    "            elif typ == 'comma':\n",
    "                self.ts.advance()  # skip comma\n",
    "            else:\n",
    "                self.ts.advance()  # skip others\n",
    "\n",
    "        self.ts.expect('rightpar')\n",
    "        #print(f\"Parsed property: {label} {values}\")\n",
    "\n",
    "    def skip_commas(self):\n",
    "        \"\"\"Advance past any comma tokens.\"\"\"\n",
    "        while self.ts.current.type == 'comma':\n",
    "            self.ts.advance()\n",
    "\n",
    "    def parse_point(self, store=True):\n",
    "        \"\"\"\n",
    "        Parse a (x y z radius [misc...]) point.\n",
    "        Stores to all_points if store=True.\n",
    "        \"\"\"\n",
    "        self.ts.expect('leftpar')\n",
    "        x = self.ts.expect('number').value\n",
    "        self.skip_commas()\n",
    "        y = self.ts.expect('number').value\n",
    "        self.skip_commas()\n",
    "        z = 0.0\n",
    "        d = 0.0\n",
    "        if self.ts.current.type == 'number':\n",
    "            z = self.ts.current.value\n",
    "            self.ts.advance()\n",
    "            self.skip_commas()\n",
    "        if self.ts.current.type == 'number':\n",
    "            d = self.ts.current.value\n",
    "            self.ts.advance()\n",
    "            self.skip_commas()\n",
    "        misc = []\n",
    "        while self.ts.current.type in ('label_', 'string'):\n",
    "            misc.append(self.ts.current.value)\n",
    "            self.ts.advance()\n",
    "        self.ts.expect('rightpar')\n",
    "\n",
    "        idx = len(self.all_points)\n",
    "        pt = Point(x, y, z, d, misc, idx)\n",
    "\n",
    "        if store == True:\n",
    "            self.all_points.append(pt)\n",
    "        return pt\n",
    "\n",
    "    def parse_tree_or_text(self, parent_id=None):\n",
    "        \"\"\"\n",
    "        Try to parse either a text block or a tree block.\n",
    "        If text parsing fails, roll back and parse as a tree.\n",
    "        \"\"\"\n",
    "        old_idx = self.ts.idx\n",
    "        try:\n",
    "            res = self.parse_text()\n",
    "            #print(f\"text detected at token {old_idx}\")\n",
    "            return {'type': 'text', 'content': res}\n",
    "        except Exception as e:\n",
    "            self.ts.idx = old_idx\n",
    "            #print(f\"text parse failed at token {old_idx}, fallback to tree: {e}\")\n",
    "            return self.parse_tree(parent_id=parent_id)\n",
    "\n",
    "    def parse_text(self):\n",
    "        \"\"\"\n",
    "        Parse a (point string) text block.\n",
    "        Returns the string content.\n",
    "        \"\"\"\n",
    "        self.ts.expect('leftpar')\n",
    "        while self.ts.current.type == 'leftpar' and self.ts.look_ahead.type in ('label_', 'set'):\n",
    "            if self.ts.look_ahead.type == 'set':\n",
    "                self.parse_set()\n",
    "            else:\n",
    "                self.parse_property()\n",
    "        pt = self.parse_point(store=False)\n",
    "        if self.ts.current.type != 'string':\n",
    "            raise ValueError(\"Text expects a string after point\")\n",
    "        content = self.ts.current.value\n",
    "        self.ts.advance()\n",
    "        self.ts.expect('rightpar')\n",
    "        return content\n",
    "\n",
    "    def parse_tree(self, parent_id=None):\n",
    "        \"\"\"\n",
    "        Parse a tree (branch structure).\n",
    "        Calls parse_properties() for initial attributes,\n",
    "        then recursively parses the main branch and sub-branches.\n",
    "        \"\"\"\n",
    "        self.ts.expect('leftpar')\n",
    "        self.parse_properties()\n",
    "        self.parse_branch(parent_id=parent_id)\n",
    "        self.ts.expect('rightpar')\n",
    "\n",
    "\n",
    "    def parse_properties(self):\n",
    "        \"\"\"\n",
    "        Parse all property blocks at the current nesting level.\n",
    "        Returns a list of parsed property values (if needed).\n",
    "        \"\"\"\n",
    "        properties = []\n",
    "        while self.ts.current.type == 'leftpar' and self.ts.look_ahead.type in ('label_', 'set'):\n",
    "            if self.ts.look_ahead.type == 'set':\n",
    "                self.parse_set()\n",
    "            else:\n",
    "                prop = self.parse_property()\n",
    "                properties.append(prop)\n",
    "        return properties\n",
    "\n",
    "    def parse_branch(self, parent_id=None):\n",
    "        \"\"\"\n",
    "        Parse a single branch (list of points), and handle branch ends and splits.\n",
    "        Registers a new Section for each branch.\n",
    "        \"\"\"\n",
    "        begin = len(self.all_points)\n",
    "        self.parse_treepoints()\n",
    "        end = len(self.all_points)\n",
    "        section = Section(sec_id=len(self.sections), sec_type=self.cur_section_type, parent_id=parent_id, parent_x = 1)\n",
    "        section.points = self.all_points[begin:end]\n",
    "        self.sections.append(section)\n",
    "        this_sec_id = section.sec_id\n",
    "        self.parse_branchend(parent_id=this_sec_id)\n",
    "\n",
    "    def parse_treepoints(self):\n",
    "        \"\"\"\n",
    "        Parse one or more point blocks (and any attached marker/spine blocks).\n",
    "        \"\"\"\n",
    "        self.parse_treepoint()\n",
    "        while self.ts.current.type == 'leftpar' and self.ts.look_ahead.type == 'number':\n",
    "            self.parse_treepoint()\n",
    "\n",
    "    def parse_treepoint(self):\n",
    "        \"\"\"\n",
    "        Parse a single tree point, and any attached markers, properties, or spines.\n",
    "        \"\"\"\n",
    "        if self.ts.look_ahead.type == 'label_':\n",
    "            # Marker or property block\n",
    "            if self.ts.look_ahead2.type == 'leftpar':\n",
    "                self.parse_marker()\n",
    "            else:\n",
    "                self.parse_property()\n",
    "        else:\n",
    "            pt = self.parse_point()\n",
    "            # Parse any attached spine block(s)\n",
    "            while self.ts.current.type == 'leftsp':  # '<'\n",
    "                print('spine')\n",
    "                self.skip_unknown_block()  # TODO: implement actual spine parsing\n",
    "\n",
    "    def parse_branchend(self, parent_id):\n",
    "        \"\"\"\n",
    "        Handle the end of a branch, including marker lists and branch splits.\n",
    "        \"\"\"\n",
    "        self.skip_commas()\n",
    "        while self.ts.current.type == 'leftpar' and self.ts.look_ahead.type == 'label_':\n",
    "            if self.ts.look_ahead2.type == 'leftpar':\n",
    "                self.parse_marker()\n",
    "            else:\n",
    "                self.parse_property()\n",
    "            self.skip_commas()\n",
    "        if self.ts.current.type == 'leftpar' or self.ts.current.type == 'label_':\n",
    "            self.parse_node(parent_id)\n",
    "\n",
    "    def parse_node(self, parent_id):\n",
    "        \"\"\"\n",
    "        Parse a branching node: either another branch or a label.\n",
    "        \"\"\"\n",
    "        if self.ts.current.type == 'leftpar':\n",
    "            self.ts.advance()\n",
    "            self.parse_split(parent_id)\n",
    "            self.ts.expect('rightpar')\n",
    "        elif self.ts.current.type == 'label_':\n",
    "            self.ts.advance()\n",
    "        else:\n",
    "            raise ValueError('node: Unexpected token')\n",
    "\n",
    "    def parse_split(self, parent_id):\n",
    "        \"\"\"\n",
    "        Parse a split node (bifurcation), indicated by bar tokens ('|').\n",
    "        Each child branch is parsed recursively.\n",
    "        \"\"\"\n",
    "        self.parse_branch(parent_id)\n",
    "        while self.ts.current.type == 'bar':\n",
    "            self.ts.advance()\n",
    "            self.parse_branch(parent_id)\n",
    "\n",
    "    def parse_spine_proc(self, base_point):\n",
    "        \"\"\"\n",
    "        Parse a <spine> block attached to a point (not fully implemented here).\n",
    "        \"\"\"\n",
    "        self.ts.expect('leftsp')\n",
    "        spine_properties = []\n",
    "        while self.ts.current.type == 'leftpar' and self.ts.look_ahead.type == 'label_':\n",
    "            prop = self.parse_property()\n",
    "            spine_properties.append(prop)\n",
    "        pt = self.parse_spine_point()\n",
    "        self.spines.append({'base_point': base_point, 'spine_tip': pt, 'properties': spine_properties})\n",
    "        self.ts.expect('rightsp')\n",
    "\n",
    "    def parse_spine_point(self):\n",
    "        \"\"\"\n",
    "        Parse a spine point (same format as a normal point).\n",
    "        \"\"\"\n",
    "        self.ts.expect('leftpar')\n",
    "        x = self.ts.expect('number').value\n",
    "        self.skip_commas()\n",
    "        y = self.ts.expect('number').value\n",
    "        self.skip_commas()\n",
    "        z = 0.0\n",
    "        d = 0.0\n",
    "        if self.ts.current.type == 'number':\n",
    "            z = self.ts.current.value\n",
    "            self.ts.advance()\n",
    "            self.skip_commas()\n",
    "        if self.ts.current.type == 'number':\n",
    "            d = self.ts.current.value\n",
    "            self.ts.advance()\n",
    "            self.skip_commas()\n",
    "        misc = []\n",
    "        while self.ts.current.type in ('label_', 'string'):\n",
    "            misc.append(self.ts.current.value)\n",
    "            self.ts.advance()\n",
    "        self.ts.expect('rightpar')\n",
    "        return {'x': x, 'y': y, 'z': z, 'd': d, 'misc': misc}\n",
    "\n",
    "    def parse_marker(self):\n",
    "        \"\"\"Skip over a marker block (not yet implemented).\"\"\"\n",
    "        self.skip_unknown_block()\n",
    "\n",
    "    def parse_set(self):\n",
    "        \"\"\"Skip over (set ...) blocks, which contain metadata or display info.\"\"\"\n",
    "        self.skip_unknown_block()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c43ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_sections(sections):\n",
    "    \"\"\"\n",
    "    Reorder the list of Section objects so that all soma sections (sec_type == 1) come first,\n",
    "    while preserving the original order within each group.\n",
    "    \"\"\"\n",
    "    soma_sec = [sec for sec in sections if sec.sec_type == 1]\n",
    "    other_sec = [sec for sec in sections if sec.sec_type != 1]\n",
    "    return soma_sec + other_sec\n",
    "\n",
    "def xy_intersect(bb1, bb2):\n",
    "    \"\"\"\n",
    "    Check whether the bounding boxes bb1 and bb2 overlap in the XY plane.\n",
    "\n",
    "    Each bounding box is (xmin, xmax, ymin, ymax).\n",
    "    Returns True if there is any overlap, False otherwise.\n",
    "    \"\"\"\n",
    "    xmin1, xmax1, ymin1, ymax1 = bb1\n",
    "    xmin2, xmax2, ymin2, ymax2 = bb2\n",
    "    return not (xmax1 < xmin2 or xmax2 < xmin1 or\n",
    "                ymax1 < ymin2 or ymax2 < ymin1)\n",
    "\n",
    "def merge_soma_sections(sections):\n",
    "    \"\"\"\n",
    "    Merge overlapping soma (sec_type==1) sections, keeping only the main stack and \n",
    "    adding any overlapping somas to its contour_stack. Non-soma sections are preserved.\n",
    "    \"\"\"\n",
    "    somas = [sec for sec in sections if sec.sec_type == 1]\n",
    "    used = set()\n",
    "    new_somas = []\n",
    "    N = len(somas)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        if i in used:\n",
    "            i += 1\n",
    "            continue\n",
    "        master = somas[i]\n",
    "        bb1 = master.bbox_xy\n",
    "        for j in range(i+1, N):\n",
    "            if j in used:\n",
    "                continue\n",
    "            cand = somas[j]\n",
    "            bb2 = cand.bbox_xy\n",
    "            if xy_intersect(bb1, bb2):\n",
    "                master.contour_stack.append(cand)\n",
    "                used.add(j)\n",
    "        new_somas.append(master)\n",
    "        i += 1\n",
    "    # Reassemble sections: only keep the main soma stacks and all other sections\n",
    "    new_sec = [sec for sec in sections if sec.sec_type != 1]\n",
    "    new_sec[:0] = new_somas  # Insert somas at the beginning\n",
    "    return new_sec\n",
    "\n",
    "def reindex_sections(sections):\n",
    "    \"\"\"\n",
    "    After reordering or merging sections, update the sec_id and parent_id of all sections\n",
    "    to ensure consistency. Returns the updated list.\n",
    "    \"\"\"\n",
    "    old2new = {}\n",
    "    for new_id, sec in enumerate(sections):\n",
    "        old2new[sec.sec_id] = new_id\n",
    "        sec.sec_id = new_id\n",
    "    for sec in sections:\n",
    "        if sec.parent_id is not None:\n",
    "            sec.parent_id = old2new.get(sec.parent_id, None)\n",
    "    return sections\n",
    "\n",
    "def remove_duplicate_points(sections):\n",
    "    \"\"\"\n",
    "    Remove duplicate points from each section. A duplicate is defined as a point with\n",
    "    exactly the same (x, y, z, d) as a previously encountered point in the same section.\n",
    "    Only the first occurrence is kept. Prints a warning for each removal.\n",
    "    \"\"\"\n",
    "    for section in sections:\n",
    "        unique_pts = []\n",
    "        seen = set()\n",
    "        for pt in section.points:\n",
    "            key = (pt.x, pt.y, pt.z, pt.d)\n",
    "            if key not in seen:\n",
    "                unique_pts.append(pt)\n",
    "                seen.add(key)\n",
    "            else:\n",
    "                print(f\"Warning: Section {section.sec_id} has duplicate point ({pt.x}, {pt.y}, {pt.z}, {pt.d}), removed.\")\n",
    "        section.points = unique_pts\n",
    "\n",
    "def ensure_section_continuity(sections):\n",
    "    \"\"\"\n",
    "    Ensure continuity between each section and its parent:\n",
    "    If the last point of the parent section does not match the first point of the child section,\n",
    "    insert a copy of the parent's last point at the beginning of the child section.\n",
    "    This only applies to non-soma parent sections.\n",
    "    \"\"\"\n",
    "    id2section = {sec.sec_id: sec for sec in sections}\n",
    "    for sec in sections:\n",
    "        if sec.parent_id is None:\n",
    "            continue\n",
    "        parent = id2section[sec.parent_id]\n",
    "        if parent.sec_type == 1:  # skip soma\n",
    "            continue\n",
    "        if not sec.points or not parent.points:\n",
    "            continue\n",
    "        parent_last = parent.points[-1]\n",
    "        child_first = sec.points[0]\n",
    "        if parent_last != child_first:\n",
    "            new_point = Point(\n",
    "                x=parent_last.x,\n",
    "                y=parent_last.y,\n",
    "                z=parent_last.z,\n",
    "                d=child_first.d,\n",
    "                misc=child_first.misc,\n",
    "                idx=None,\n",
    "            )\n",
    "            sec.points.insert(0, new_point)\n",
    "            # print(f\"Added continuity point from parent {parent.sec_id} to child {sec.sec_id}\")\n",
    "\n",
    "def validate_soma_stack(main_soma_section, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Check that the main soma section and its contour_stack meet the following criteria:\n",
    "      1. All points within each layer (section) have identical z value (within tolerance);\n",
    "      2. The z values across layers are strictly monotonic (increasing or decreasing).\n",
    "    Raises ValueError if not satisfied.\n",
    "    \"\"\"\n",
    "    stack = [main_soma_section] + list(getattr(main_soma_section, 'contour_stack', []))\n",
    "\n",
    "    # Check all z values in each section are the same\n",
    "    for idx, sec in enumerate(stack):\n",
    "        z_vals = [p.z for p in sec.points]\n",
    "        if not z_vals:\n",
    "            raise ValueError(f\"[SOMA CHECK] Section {idx} in soma stack is empty\")\n",
    "        z0 = z_vals[0]\n",
    "        if not all(abs(z - z0) < tol for z in z_vals):\n",
    "            raise ValueError(f\"[SOMA CHECK] Contour {idx} z-values not constant: {z_vals}\")\n",
    "\n",
    "    # Check monotonicity of the stack in z\n",
    "    z_stack = [sec.points[0].z for sec in stack if sec.points]\n",
    "    dzs = [z_stack[i+1] - z_stack[i] for i in range(len(z_stack)-1)]\n",
    "    if all(d > tol for d in dzs):\n",
    "        return True  # strictly increasing\n",
    "    if all(d < -tol for d in dzs):\n",
    "        return True  # strictly decreasing\n",
    "    raise ValueError(f\"[SOMA CHECK] Contour stack z-values not monotonic: {z_stack}\")\n",
    "\n",
    "def validate_soma_stack_main(sections):\n",
    "    \"\"\"\n",
    "    Validate all soma stacks in the given sections list.\n",
    "    \"\"\"\n",
    "    for sec in sections:\n",
    "        if sec.sec_type == 1:\n",
    "            validate_soma_stack(sec)\n",
    "\n",
    "def interpolate_soma_stack(main_soma_section, n_sample=21):\n",
    "    \"\"\"\n",
    "    Perform principal-axis interpolation along the soma contour stack, sampling n_sample points.\n",
    "    Replace the points in main_soma_section with the new samples, and return the new point list.\n",
    "    \"\"\"\n",
    "    # Gather all points from the main section and its contour stack\n",
    "    stack = [main_soma_section] + list(getattr(main_soma_section, 'contour_stack', []))\n",
    "    all_points = []\n",
    "    for sec in stack:\n",
    "        all_points.extend(sec.points)\n",
    "    if len(all_points) < 3:\n",
    "        raise ValueError(\"Not enough points for interpolation\")\n",
    "    # Compute principal axis by PCA (first eigenvector of covariance matrix)\n",
    "\n",
    "def connect_to_soma(sections, buffer=0.5, verbose=True):\n",
    "    \"\"\"\n",
    "    Automatically connect all dangling (parentless, non-soma) sections to the appropriate soma.\n",
    "    - First, tries to match the section's root point to a \"loose\" bounding box (bbox) around each soma.\n",
    "    - If not inside any soma bbox, connects to the center of the nearest soma.\n",
    "    - Supports both contour_stack and regular point-cloud soma representations.\n",
    "\n",
    "    Args:\n",
    "        sections (list): List of Section objects (must include both soma and branches).\n",
    "        buffer (float): Extra margin for bbox test.\n",
    "        verbose (bool): If True, print connection info.\n",
    "\n",
    "    Returns:\n",
    "        unmatched (list): Dangling sections not inside any soma bbox, but auto-connected to the nearest soma.\n",
    "    \"\"\"\n",
    "    # 1. Extract all soma sections\n",
    "    soma_secs = [sec for sec in sections if sec.sec_type == 1]\n",
    "\n",
    "    # 2. Precompute centers and bounding boxes for all somas\n",
    "    soma_centers = []\n",
    "    for soma_sec in soma_secs:\n",
    "        if hasattr(soma_sec, \"contour_stack\") and soma_sec.contour_stack:\n",
    "            center = soma_sec.stk_center     # Use contour stack center\n",
    "            bbox_xy = soma_sec.stk_bbox_xy   # Use stack bbox\n",
    "        else:\n",
    "            center = soma_sec.center         # Use point cloud mean\n",
    "            bbox_xy = soma_sec.bbox_xy\n",
    "        soma_centers.append(center)\n",
    "        soma_sec._bbox_xy = bbox_xy  # Optionally cache on object\n",
    "\n",
    "    # 3. Gather all dangling non-soma sections\n",
    "    dangling_secs = [sec for sec in sections if sec.parent_id is None and sec.sec_type != 1]\n",
    "    unmatched = []\n",
    "\n",
    "    # 4. Try to connect each dangling section to a soma\n",
    "    for dangling_sec in dangling_secs:\n",
    "        if not dangling_sec.points:\n",
    "            continue\n",
    "        x0, y0 = dangling_sec.points[0].x, dangling_sec.points[0].y\n",
    "        found = False\n",
    "        for i, soma_sec in enumerate(soma_secs):\n",
    "            # Use appropriate bbox for this soma\n",
    "            if hasattr(soma_sec, \"contour_stack\") and soma_sec.contour_stack:\n",
    "                xmin, xmax, ymin, ymax = soma_sec.stk_bbox_xy\n",
    "            else:\n",
    "                xmin, xmax, ymin, ymax = soma_sec.bbox_xy\n",
    "            loose_xmin, loose_xmax = xmin-buffer, xmax+buffer\n",
    "            loose_ymin, loose_ymax = ymin-buffer, ymax+buffer\n",
    "            # Check if root point falls inside the (loosened) bbox\n",
    "            if loose_xmin <= x0 <= loose_xmax and loose_ymin <= y0 <= loose_ymax:\n",
    "                if verbose:\n",
    "                    print(f\"{dangling_sec.sec_id} falls inside loose bbox of soma {soma_sec.sec_id}\")\n",
    "                dangling_sec.parent_id = soma_sec.sec_id\n",
    "                dangling_sec.parent_x = 0.5  # Attach to soma center (for NEURON-style models)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            # Not inside any soma bbox; connect to nearest soma center\n",
    "            min_dist = float('inf')\n",
    "            min_idx = None\n",
    "            z0 = dangling_sec.points[0].z\n",
    "            for i, center in enumerate(soma_centers):\n",
    "                cx, cy, cz = center\n",
    "                dist = ((x0-cx)**2 + (y0-cy)**2 + (z0-cz)**2)**0.5\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_idx = i\n",
    "            nearest_soma = soma_secs[min_idx]\n",
    "            if verbose:\n",
    "                print(f\"!!! Section {dangling_sec.sec_id} root ({x0:.2f},{y0:.2f}) not inside any soma bbox; connected to nearest soma ({nearest_soma.sec_id})\")\n",
    "            dangling_sec.parent_id = nearest_soma.sec_id\n",
    "            dangling_sec.parent_x = 0.5\n",
    "            unmatched.append(dangling_sec)  # Store for possible manual inspection\n",
    "\n",
    "    if verbose and unmatched:\n",
    "        print(f\"\\n{len(unmatched)} dangling branches were not inside any soma bbox, but were auto-connected to the nearest soma.\")\n",
    "    return unmatched  # List of \"rescued\" branches for optional post-processing\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "000d1e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contourcenter(points, num=101):\n",
    "    \"\"\"\n",
    "    Uniformly resample a 3D contour by arclength and return centroid.\n",
    "\n",
    "    Args:\n",
    "        points: list of Point (with .x, .y, .z)\n",
    "        num: number of resample points\n",
    "\n",
    "    Returns:\n",
    "        mean: ndarray, shape (3,), centroid (mean_x, mean_y, mean_z)\n",
    "        x_new, y_new, z_new: ndarray, resampled coordinates (length=num)\n",
    "    \"\"\"\n",
    "    x = np.array([p.x for p in points])\n",
    "    y = np.array([p.y for p in points])\n",
    "    z = np.array([p.z for p in points])\n",
    "    seglens = np.sqrt(np.diff(x)**2 + np.diff(y)**2 + np.diff(z)**2)\n",
    "    perim = np.zeros(len(x))\n",
    "    perim[1:] = np.cumsum(seglens)\n",
    "    d_uniform = np.linspace(0, perim[-1], num)\n",
    "    x_new = np.interp(d_uniform, perim, x)\n",
    "    y_new = np.interp(d_uniform, perim, y)\n",
    "    z_new = np.interp(d_uniform, perim, z)\n",
    "    mean = np.array([x_new.mean(), y_new.mean(), z_new.mean()])\n",
    "    return mean, x_new, y_new, z_new\n",
    "\n",
    "def neuron_soma_axis_sampling(\n",
    "    points, \n",
    "    n_samples=21,\n",
    "    arclength_resample=101,\n",
    "    plot_axes=False,\n",
    "    plot_sides=False,\n",
    "    plot_interp=False,\n",
    "    plot_radii=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Fit a closed soma contour, perform convex filtering and main axis interpolation,\n",
    "    and return 21 spatial positions along the main axis and their diameter.\n",
    "\n",
    "    Args:\n",
    "        points: List of point objects with .x, .y attributes (2D contour, closed).\n",
    "        n_samples: Number of axis points to sample (default 21).\n",
    "        arclength_resample: How many arclength-resampled points for contour smoothing.\n",
    "        plot_axes: If True, plot the main axis and resampling.\n",
    "        plot_sides: If True, plot convex filtered two sides.\n",
    "        plot_interp: If True, plot the 21 sampled axis points.\n",
    "        plot_radii: If True, plot side/interpolated radii/diameter.\n",
    "\n",
    "    Returns:\n",
    "        XY_interp: (n_samples, 2) array of axis-sampled positions.\n",
    "        diam_interp: (n_samples,) array of diameter at each axis sample.\n",
    "    \"\"\"\n",
    "    # Step 1: Arclength uniform resampling and centroid\n",
    "    mean, x_new, y_new, _ = contourcenter(points, num=arclength_resample)\n",
    "    mean = mean[:-1] ## no z value \n",
    "    # Step 2: PCA for principal/minor axes\n",
    "    pts = np.stack([x_new, y_new], axis=1)\n",
    "    pts_centered = pts - mean\n",
    "    cov = np.cov(pts_centered, rowvar=False)\n",
    "    _, eigvecs = np.linalg.eigh(cov)\n",
    "    major = eigvecs[:, 1]\n",
    "    minor = eigvecs[:, 0]\n",
    "    if major[np.argmax(np.abs(major))] < 0:\n",
    "        major = -major\n",
    "    major = major / np.linalg.norm(major)\n",
    "    minor = minor / np.linalg.norm(minor)\n",
    "\n",
    "    # Step 3: Project all points onto axes\n",
    "    d = (pts - mean) @ major\n",
    "    rad = (pts - mean) @ minor\n",
    "\n",
    "    # if plot_axes:\n",
    "    #     plt.figure(figsize=(7,7))\n",
    "    #     plt.plot(x, y, 'o-', label='Raw points', alpha=0.5)\n",
    "    #     plt.plot(x_new, y_new, 'r.', label='Arclength-resample', alpha=0.6)\n",
    "    #     plt.scatter(mean[0], mean[1], c='b', s=100, label='Center')\n",
    "    #     plt.plot([mean[0], mean[0] + 0.6*(d.max()-d.min())*major[0]],\n",
    "    #              [mean[1], mean[1] + 0.6*(d.max()-d.min())*major[1]],\n",
    "    #              'g-', lw=3, label='Principal axis')\n",
    "    #     plt.plot([mean[0], mean[0] + 0.3*(d.max()-d.min())*minor[0]],\n",
    "    #              [mean[1], mean[1] + 0.3*(d.max()-d.min())*minor[1]],\n",
    "    #              'c-', lw=3, label='Minor axis')\n",
    "    #     plt.axis('equal')\n",
    "    #     plt.legend()\n",
    "    #     plt.title('Mean, principal/minor axis, resampled')\n",
    "    #     plt.show()\n",
    "\n",
    "    # Step 4: Split contour into two convex sides and filter\n",
    "    def rotate(arr, k):\n",
    "        return np.concatenate([arr[k:], arr[:k]])\n",
    "\n",
    "    def keep_strictly_monotonic(x, y, increasing=True, tol=1e-8):\n",
    "        keep_idx = [0]\n",
    "        for i in range(1, len(x)):\n",
    "            if increasing:\n",
    "                if x[i] > x[keep_idx[-1]] + tol:\n",
    "                    keep_idx.append(i)\n",
    "            else:\n",
    "                if x[i] < x[keep_idx[-1]] - tol:\n",
    "                    keep_idx.append(i)\n",
    "        return x[keep_idx], y[keep_idx]\n",
    "\n",
    "    imax = np.argmax(d)\n",
    "    imin = np.argmin(d)\n",
    "    d_rot = rotate(d, imax)\n",
    "    rad_rot = rotate(rad, imax)\n",
    "    pts_rot = rotate(pts, imax)\n",
    "    imin_new = np.where(d_rot == d[imin])[0][0]\n",
    "\n",
    "    d_side1 = d_rot[:imin_new][::-1]\n",
    "    rad_side1 = rad_rot[:imin_new][::-1]\n",
    "    pts_side1 = pts_rot[:imin_new][::-1]\n",
    "    d_side2 = d_rot[imin_new:]\n",
    "    rad_side2 = rad_rot[imin_new:]\n",
    "    pts_side2 = pts_rot[imin_new:]\n",
    "\n",
    "    inc1 = len(d_side1) > 1 and (d_side1[1] > d_side1[0])\n",
    "    inc2 = len(d_side2) > 1 and (d_side2[1] > d_side2[0])\n",
    "    d_side1_new, rad_side1_new = keep_strictly_monotonic(d_side1, rad_side1, increasing=inc1)\n",
    "    d_side2_new, rad_side2_new = keep_strictly_monotonic(d_side2, rad_side2, increasing=inc2)\n",
    "    pts_side1_new = mean[None, :] + d_side1_new[:, None] * major[None, :] + rad_side1_new[:, None] * minor[None, :]\n",
    "    pts_side2_new = mean[None, :] + d_side2_new[:, None] * major[None, :] + rad_side2_new[:, None] * minor[None, :]\n",
    "\n",
    "    # if plot_sides:\n",
    "    #     plt.figure(figsize=(7,7))\n",
    "    #     plt.plot(pts[:,0], pts[:,1], 'k.-', alpha=0.3, label='Contour')\n",
    "    #     plt.plot(pts_side1_new[:,0], pts_side1_new[:,1], 'ro-', label='Filtered side1')\n",
    "    #     plt.plot(pts_side2_new[:,0], pts_side2_new[:,1], 'bo-', label='Filtered side2')\n",
    "    #     plt.scatter(mean[0], mean[1], c='g', s=60, label='Center')\n",
    "    #     plt.axis('equal')\n",
    "    #     plt.title('Convex sides (filtered) in XY')\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "\n",
    "    # Step 5: Interpolate main axis (exclude endpoints)\n",
    "    d_all = np.concatenate([d_side1_new, d_side2_new])\n",
    "    d_all_sorted = np.sort(d_all)\n",
    "    d_min = d_all_sorted[1]\n",
    "    d_max = d_all_sorted[-2]\n",
    "    d_interp = np.linspace(d_min, d_max, n_samples)\n",
    "    XY_interp = mean[None, :] + d_interp[:, None] * major[None, :]\n",
    "\n",
    "    # if plot_interp:\n",
    "    #     plt.figure(figsize=(7,7))\n",
    "    #     plt.plot(pts[:,0], pts[:,1], 'k.-', alpha=0.3, label='Resampled contour')\n",
    "    #     plt.plot(XY_interp[:,0], XY_interp[:,1], 'o-', color='orange', label=f'{n_samples} axis points')\n",
    "    #     plt.scatter(mean[0], mean[1], c='g', s=80, label='Center')\n",
    "    #     plt.axis('equal')\n",
    "    #     plt.legend()\n",
    "    #     plt.title(f'{n_samples} interpolated points along major axis')\n",
    "    #     plt.show()\n",
    "\n",
    "    # Step 6: Interpolate radii for both sides, then compute diameter\n",
    "    f_rad1 = interp1d(d_side1_new, rad_side1_new, kind='linear', bounds_error=False,\n",
    "                      fill_value=(rad_side1_new[0], rad_side1_new[-1]))\n",
    "    f_rad2 = interp1d(d_side2_new, rad_side2_new, kind='linear', bounds_error=False,\n",
    "                      fill_value=(rad_side2_new[0], rad_side2_new[-1]))\n",
    "    rad1_interp = f_rad1(d_interp)\n",
    "    rad2_interp = f_rad2(d_interp)\n",
    "    diam_interp = np.abs(rad1_interp - rad2_interp)\n",
    "    # Smooth endpoints (as NEURON/HOC)\n",
    "    diam_interp[0] = (diam_interp[0] + diam_interp[1]) / 2\n",
    "    diam_interp[-1] = (diam_interp[-1] + diam_interp[-2]) / 2\n",
    "\n",
    "    XY_side1 = XY_interp + rad1_interp[:, None] * minor[None, :]\n",
    "    XY_side2 = XY_interp + rad2_interp[:, None] * minor[None, :]\n",
    "\n",
    "    # if plot_radii:\n",
    "    #     plt.figure(figsize=(8,5))\n",
    "    #     plt.plot(d_interp, rad1_interp, 'ro-', label='Side1 radius')\n",
    "    #     plt.plot(d_interp, rad2_interp, 'bo-', label='Side2 radius')\n",
    "    #     plt.plot(d_interp, diam_interp, 'ko-', label='Diameter')\n",
    "    #     plt.xlabel('d (on major axis)')\n",
    "    #     plt.ylabel('radius / diameter')\n",
    "    #     plt.title('Interpolated radii and diameter at axis points')\n",
    "    #     plt.legend()\n",
    "    #     plt.grid()\n",
    "    #     plt.show()\n",
    "\n",
    "    #     plt.figure(figsize=(7,7))\n",
    "    #     plt.plot(pts[:,0], pts[:,1], 'k.-', alpha=0.2, label='Original')\n",
    "    #     plt.plot(XY_interp[:,0], XY_interp[:,1], 'o-', color='orange', label='Axis points')\n",
    "    #     plt.plot(XY_side1[:,0], XY_side1[:,1], 'ro-', label='Side1 boundary')\n",
    "    #     plt.plot(XY_side2[:,0], XY_side2[:,1], 'bo-', label='Side2 boundary')\n",
    "    #     plt.axis('equal')\n",
    "    #     plt.legend()\n",
    "    #     plt.title('Axis points & two side boundaries')\n",
    "    #     plt.show()\n",
    "\n",
    "    return XY_interp, diam_interp\n",
    "\n",
    "def approximate_contour_by_circle(points, num=101):\n",
    "    \"\"\"\n",
    "    points: list of Point(x, y, z)\n",
    "    num: int, number of points for arclength-uniform resampling\n",
    "\n",
    "    Returns:\n",
    "        center: (x, y, z) tuple, centroid of resampled contour\n",
    "        avg_radius: float, averaged radius (robust hybrid)\n",
    "    \"\"\"\n",
    "    n = len(points)\n",
    "    if n < 2:\n",
    "        raise ValueError(\"At least two points required\")\n",
    "    # Use arclength-resampled centroid (robust against uneven input points)\n",
    "    mean, x_new, y_new, z_new = contourcenter(points, num=num)\n",
    "    mean_radius = np.mean(\n",
    "        np.sqrt((x_new - mean[0])**2 + (y_new - mean[1])**2 + (z_new - mean[2])**2)\n",
    "    )\n",
    "    perim = np.sum(np.sqrt(np.diff(x_new)**2 + np.diff(y_new)**2 + np.diff(z_new)**2))\n",
    "    perim_radius = perim / (2 * np.pi)\n",
    "    diam =  perim_radius + mean_radius\n",
    "    return mean, diam\n",
    "\n",
    "def contourstack2centroid(section, num=101):\n",
    "    \"\"\"\n",
    "    Approximate each contour (main + contour_stack) as a circle,\n",
    "    return all centers (x, y, z) and diameters as lists.\n",
    "    \n",
    "    Args:\n",
    "        section: a Section object with .points (main contour) and .contour_stack (list of contour, each a list of Point)\n",
    "        num: number of points for arclength-uniform resampling\n",
    "        verbose: print area info if True\n",
    "\n",
    "    Returns:\n",
    "        xs, ys, zs: list of float, all centers for each layer\n",
    "        diams: list of float, all diameters for each layer\n",
    "    \"\"\"\n",
    "    xs, ys, zs, diams = [], [], [], []\n",
    "\n",
    "    # 1. 主contour\n",
    "    mean, diameter = approximate_contour_by_circle(section.points, num=num)\n",
    "    xs.append(mean[0]); ys.append(mean[1]); zs.append(mean[2]); diams.append(diameter)\n",
    "\n",
    "    # 2. 每个contour_stack\n",
    "    for contour in getattr(section, \"contour_stack\", []):\n",
    "        if contour:\n",
    "            mean, diameter = approximate_contour_by_circle(contour, num=num)\n",
    "            xs.append(mean[0]); ys.append(mean[1]); zs.append(mean[2]); diams.append(diameter)\n",
    "\n",
    "    return xs, ys, zs, diams\n",
    "\n",
    "\n",
    "def replace_soma_with_axis_sampling(sections, n_samples=21, **plot_kwargs):\n",
    "    \"\"\"\n",
    "    For each soma section in `sections`, replace its points with axis-based samples:\n",
    "    - If the soma section has only a single contour (no contour_stack), use `neuron_soma_axis_sampling`.\n",
    "    - If the soma section is multi-layer (contour_stack not empty), use stack-based axis sampling (`contourstack2centroid`).\n",
    "    - All other sections are left unchanged.\n",
    "\n",
    "    Args:\n",
    "        sections: list of Section objects (must have sec_type, points, optionally contour_stack).\n",
    "        n_samples: for single-contour soma, number of axis-sampled points to generate.\n",
    "        plot_kwargs: extra switches for neuron_soma_axis_sampling.\n",
    "\n",
    "    Returns:\n",
    "        sections: updated in place.\n",
    "    \"\"\"\n",
    "    for sec in sections:\n",
    "        if getattr(sec, 'sec_type', None) == 1:\n",
    "            # Case 1: Single-layer soma (no contour_stack)\n",
    "            if not getattr(sec, 'contour_stack', []):\n",
    "                XY, diam = neuron_soma_axis_sampling(\n",
    "                    sec.points, \n",
    "                    n_samples=n_samples, \n",
    "                    **plot_kwargs\n",
    "                )\n",
    "                sec.points = [\n",
    "                    Point(\n",
    "                        x=XY[i,0], y=XY[i,1], \n",
    "                        z=sec.points[0].z if sec.points else 0.0, \n",
    "                        d=float(diam[i]), misc=[], idx=i\n",
    "                    )\n",
    "                    for i in range(len(diam))\n",
    "                ]\n",
    "            # Case 2: Multi-layer soma (contour_stack present)\n",
    "            else:\n",
    "                xs, ys, zs, diams = contourstack2centroid(sec, num=n_samples)\n",
    "                sec.points = [\n",
    "                    Point(\n",
    "                        x=xs[i], y=ys[i], z=zs[i],\n",
    "                        d=float(diams[i]), misc=[], idx=i\n",
    "                    )\n",
    "                    for i in range(len(xs))\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb5cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file):\n",
    "    ## parse the tokens from .asc\n",
    "    tokens = []\n",
    "    with open(file) as f: \n",
    "        for i, line in enumerate(f):\n",
    "            tokens.extend(tokenize_asc_line(line, i+1))\n",
    "    tokens.append(Token('eof', '', i+2))\n",
    "\n",
    "    parser = Parser(tokens)\n",
    "    parser.parse()\n",
    "\n",
    "    ## preprocess \n",
    "    remove_duplicate_points(parser.sections)\n",
    "    ## sort and reindex soma sec\n",
    "    parser.sections = sort_sections(parser.sections)\n",
    "    parser.sections = merge_soma_sections(parser.sections)\n",
    "    parser.sections = reindex_sections(parser.sections)\n",
    "    ## make sure the continuety of non-soma sec\n",
    "    ensure_section_continuity(parser.sections)\n",
    "    ## make sure soma stack make sense\n",
    "    validate_soma_stack_main(parser.sections)\n",
    "    ## connect to soma\n",
    "    unmatched  = connect_to_soma(parser.sections, buffer=0.5, verbose=False)\n",
    "    ## resampling soma points\n",
    "    replace_soma_with_axis_sampling(parser.sections, n_samples=21)\n",
    "    return parser.sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ce47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = main('pair-140514-C2-1_split_1.asc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00eb9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('golgi_connection.json', 'r') as file:\n",
    "    mor_connection = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abb55795",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_neuron = [\n",
    "    [int(child.split('_')[-1]), int(parent.split('_')[-1]), float(parentx)]\n",
    "    for child, parent, parentx in mor_connection\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "481169eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.044344624335736e-08\n"
     ]
    }
   ],
   "source": [
    "mor_info = np.load('golgi_info.npy',allow_pickle= True).item()\n",
    "points_h = [v['points'] for k, v in mor_info.items()]\n",
    "points_asc = [\n",
    "    [np.array([p.x, p.y, p.z, p.d]) for p in sec.points]\n",
    "    for sec in sections \n",
    "]\n",
    "\n",
    "diff_list = []\n",
    "for i in range(len(points_asc)):\n",
    "    if i >=0:\n",
    "        #diff = len(points_asc[i]) - len(points_h[i])\n",
    "        diff = np.mean(points_h[i] - points_asc[i])\n",
    "        diff_list.append(diff)\n",
    "        #print(diff)\n",
    "print(np.mean(np.array(diff_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "004e8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_points = sections [0].points\n",
    "interp_pts = np.array([\n",
    "    [-22.992474,  3.054222],\n",
    "    [-22.364834,  3.672989],\n",
    "    [-21.737196,  4.291757],\n",
    "    [-21.109558,  4.910525],\n",
    "    [-20.481918,  5.529293],\n",
    "    [-19.854280,  6.148060],\n",
    "    [-19.226641,  6.766828],\n",
    "    [-18.599003,  7.385595],\n",
    "    [-17.971365,  8.004363],\n",
    "    [-17.343725,  8.623131],\n",
    "    [-16.716087,  9.241899],\n",
    "    [-16.088448,  9.860666],\n",
    "    [-15.460810, 10.479434],\n",
    "    [-14.833171, 11.098202],\n",
    "    [-14.205533, 11.716969],\n",
    "    [-13.577894, 12.335737],\n",
    "    [-12.950255, 12.954504],\n",
    "    [-12.322617, 13.573272],\n",
    "    [-11.694978, 14.192039],\n",
    "    [-11.067340, 14.810807],\n",
    "    [-10.439701, 15.429575],\n",
    "])\n",
    "\n",
    "neuron_diam = np.array([ 3.47840428,  6.40318727,  9.56432724, 11.33119488, 12.23450279,\n",
    "       12.34788513, 12.18955517, 12.22898865, 12.02369404, 11.81756783,\n",
    "       11.76656723, 11.80428028, 11.83661652, 11.48097897, 10.98648643,\n",
    "       10.49935722,  9.89296722,  9.28091908,  8.5637455 ,  7.22931767,\n",
    "        3.99473667])\n",
    "\n",
    "XY, diam = neuron_soma_axis_sampling(test_points, n_samples=21, plot_axes=False, plot_sides=False, plot_interp=False, plot_radii=False)\n",
    "diff_xyz = interp_pts - XY\n",
    "diff_diam = diam - neuron_diam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "braincell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
